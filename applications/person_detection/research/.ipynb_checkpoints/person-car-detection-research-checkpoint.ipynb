{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I present the research and development of a multi-label neural network model for object detection(not localization, just present or not present). Specifically, the targeted devices for deployment are microcontrollers, so the model must be constrained.\n",
    "\n",
    "My intention is to train a small model to detect whether a person, a car, or both are present in an image. Note that **the following scripts can be used for different classes and applications**. Please check the rest of the repository to learn more and see the deployment phase to different microcontrollers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import all the libraries we will need and set some configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common imports\n",
    "import numpy as np\n",
    "import time, os, sys, zipfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "#Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "#Reproducibility\n",
    "random.seed(23)\n",
    "np.random.seed(23)\n",
    "tf.random.set_seed(23)\n",
    "\n",
    "#Plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "#Dataset utilities\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the last import, we will use the `pycocotools` library to build our data pipeline.\n",
    "\n",
    "For reproducibility, here is some info of what I have installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS: nt\n",
      "Python version: 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]\n",
      "Numpy: 1.18.5\n",
      "Tensorflow: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Record package versions for reproducibility\n",
    "print(\"OS: {}\".format(os.name))\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"Numpy: {}\".format(np.__version__))\n",
    "print(\"Tensorflow: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's set up a couple of global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \".\"\n",
    "DATA_DIR = os.path.join(ROOT_PATH, \"data\")\n",
    "coco_year = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations\n",
    "\n",
    "Unfortunately, the ImageNet dataset does not contain a class \"person\", so instead we can use the [COCO dataset](https://cocodataset.org/#home). On top of having the person class, the dataset name means \"Common Objects in COntext\" so, in theory, the images will be a better representation of the reality the model will be exposed to in the final application.\n",
    "\n",
    "The visual wake word dataset would have been useful for this application if it could be extended to more than one foreground class, but it served as inspiration.\n",
    "\n",
    "Finally, as we will see, retraining the final model(fine tuning) with captured images from the environment and device the model will be deployed on will help improve its performance.\n",
    "\n",
    "## Execution\n",
    "The COCO dataset is designed for a variety of deep learning applications, but classification is not one of them, so we will need to play with the COCO API to get the data in the format we want for **multi-label** classification.\n",
    "\n",
    "The following class provides an easy interface to do just that, from downloading the original annotations and images, to the conversion to a `tf.dataset` or `torch.utils.data.DataLoader` object for an optimized data pipeline. The information and comments provided should be enough to understand its API, but do not forget to check the rest of the repository for the standalone script and more usage information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCO_MLC():\n",
    "    \"\"\"\n",
    "    COCO_C aims to convert the original COCO dataset into a classification problem. This approach\n",
    "    makes the task easier and smaller models can be fit. Useful for constrained devices.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Folder where all data will be downloaded\n",
    "        year: COCO dataset year\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, year=\"2017\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.year = str(year)\n",
    "        self.ANN_FOLDER = os.path.join(self.data_dir, \"annotations\")\n",
    "        self.split_names = [\"instances_train{}.json\".format(self.year),\n",
    "                            \"instances_val{}.json\".format(self.year)]\n",
    "        self.coco_categories = []\n",
    "        self.classes = []\n",
    "        \n",
    "        if not os.path.exists(self.data_dir):\n",
    "            print(\"data_dir does not exist, creating directory now\")\n",
    "            os.makedirs(self.data_dir)\n",
    "            \n",
    "    def download_annotations(self, delete_zip=False):\n",
    "        \"\"\"\n",
    "        Create annotations folder in data dir and download `year` COCO annotations.\n",
    "\n",
    "        \"\"\"\n",
    "        ANN_ZIP_FILE_PATH = os.path.join(self.ANN_FOLDER, \"annotations_train{}.zip\".format(self.year))\n",
    "        ANN_URL = r\"http://images.cocodataset.org/annotations/annotations_trainval{}.zip\".format(self.year)\n",
    "        \n",
    "        if not os.path.exists(self.ANN_FOLDER):\n",
    "            print(\"Creating annotations folder: {}\".format(self.ANN_FOLDER))\n",
    "            os.makedirs(self.ANN_FOLDER)\n",
    "        if not os.path.exists(ANN_ZIP_FILE_PATH):\n",
    "            print(\"Downloading annotations...\")\n",
    "            with urllib.request.urlopen(ANN_URL) as resp, open(ANN_ZIP_FILE_PATH, 'wb') as out:\n",
    "                shutil.copyfileobj(resp, out)\n",
    "            print (\"... done downloading.\")\n",
    "        \n",
    "        print(\"Unzipping {}\".format(ANN_ZIP_FILE_PATH))\n",
    "        with zipfile.ZipFile(ANN_ZIP_FILE_PATH,\"r\") as zip_ref:\n",
    "            for split in self.split_names:\n",
    "                split_zip_path = os.path.join(\"annotations\", split)\n",
    "                split_zip_path = split_zip_path.replace(\"\\\\\", \"/\") # Needed by zipfile\n",
    "                zip_ref.extract(split_zip_path, self.data_dir)\n",
    "        print (\"... done unzipping\")\n",
    "\n",
    "        if delete_zip:\n",
    "            print(\"Removing original zip file...\")\n",
    "            os.remove(ANN_ZIP_FILE_PATH)\n",
    "        print(\"... done\")\n",
    "        \n",
    "        # Let's create a list of categories for the user to check\n",
    "        val_ann_file = os.path.join(self.ANN_FOLDER, self.split_names[1])\n",
    "        coco_obj = COCO(val_ann_file)\n",
    "        cats = coco_obj.loadCats(coco_obj.getCatIds())\n",
    "        self.coco_categories = [cat['name'] for cat in cats]\n",
    "        \n",
    "        print(\"Download annotations done\")\n",
    "        \n",
    "    def download_images(self, classes=[], threshold_area=0.005, only_length=False, max_length=None,\n",
    "                       add_negative_class=True, neg_classes=[]):\n",
    "        \"\"\"\n",
    "        Download images from the desired classes and store them in different folders. For example, \n",
    "        after running `download_annotations` and then this function with \"person\" and \"car\" classes,\n",
    "        we end up with a tree that looks as follows:\n",
    "\n",
    "        -- data_dir\n",
    "            |-- annotations\n",
    "            |-- train\n",
    "            |   |-- car\n",
    "            |   `-- person\n",
    "            `-- val\n",
    "                |-- car\n",
    "                `-- person\n",
    "\n",
    "        \n",
    "        Args:\n",
    "            classes: classes from which to download images\n",
    "            threshold_area: mininum area percentage the desired foreground object\n",
    "                            must have to be downloaded\n",
    "            only_length: when True, it does not download the images\n",
    "            max_length: the max number of COCO annotations to scan for. By default scans all.\n",
    "                        This is useful when you want to get only a few downloaded images when \n",
    "                        trying new 'threshold_area' values. Note that in COCO dataset there is \n",
    "                        usually more than one annotation per image, so this parameter is not \n",
    "                        the amount of images to be downloaded, although the more annotations \n",
    "                        you scan, the more images will be downloaded, allsatisfying the \n",
    "                        `threshold_area` constraint.\n",
    "            add_negative_class: when True, it downloads images that do not correspond to any \n",
    "                        of the `classes`. It downloads as many as needed to have a balanced \n",
    "                        dataset.\n",
    "            neg_classes: COCO categories that will end up in the negative class. If empty,\n",
    "                        it will use all remaining categories not in `classes`. It will\n",
    "                        contain equal amount of each negative class(balanced).\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary with keys \"train\" and \"val\" that contains per split per\n",
    "            category data length\n",
    "        \n",
    "        \"\"\"\n",
    "        if not classes:\n",
    "            return\n",
    "        self.classes = classes\n",
    "        split_dirs = [\"train\", \"val\"]\n",
    "        data_lens = {\"train\":[], \"val\":[]}\n",
    "        \n",
    "        for split, split_dir in zip(self.split_names, split_dirs):\n",
    "            split_path = os.path.join(self.ANN_FOLDER, split)\n",
    "            coco = COCO(split_path) # Should we make these class attributes? What about memory?\n",
    "            cat_ids = coco.getCatIds(self.classes)\n",
    "            for cat_id, cat_name in zip(cat_ids, self.classes):\n",
    "                print(\"Downloading {} data for {} category\".format(split_dir, cat_name))\n",
    "                cat_path = os.path.join(self.data_dir, split_dir ,cat_name)\n",
    "                os.makedirs(cat_path, exist_ok=True)\n",
    "                \n",
    "                # load annotations\n",
    "                ann_ids = coco.getAnnIds(catIds=[cat_id])[:max_length]\n",
    "                anns = coco.loadAnns(ann_ids)\n",
    "                \n",
    "                # Check area threshold and create \"img.json\" list\n",
    "                imgs_data=[]\n",
    "                for ann in anns:\n",
    "                    img_id = ann[\"image_id\"]\n",
    "                    img = coco.loadImgs([img_id])[0]\n",
    "                    img_area = img[\"height\"] * img[\"width\"]\n",
    "                    normalized_object_area = ann[\"area\"]/img_area\n",
    "                    if normalized_object_area > threshold_area:\n",
    "                        if img not in imgs_data:\n",
    "                            imgs_data.append(img)\n",
    "                \n",
    "                N = len(imgs_data)\n",
    "                data_lens[split_dir].append(N)\n",
    "                if only_length:\n",
    "                    continue\n",
    "                    \n",
    "                # Download images\n",
    "                tic = time.time()\n",
    "                for i, img in enumerate(imgs_data):\n",
    "                    fname = os.path.join(cat_path, img['file_name'])\n",
    "                    if not os.path.exists(fname):\n",
    "                        urllib.request.urlretrieve(img['coco_url'], fname)\n",
    "                    print('Downloaded {}/{} images (t={:.2f}s)'.format(i+1, N, time.time()- tic), end=\"\\r\")\n",
    "                print(\"\\n\")\n",
    "                \n",
    "            if add_negative_class:\n",
    "                print(\"Downloading {} data for negative category\".format(split_dir, cat_name))\n",
    "                neg_path = os.path.join(self.data_dir, split_dir, \"negative\")\n",
    "                os.makedirs(neg_path, exist_ok=True)\n",
    "                \n",
    "                # Obtain the number of images per class to get a balanced negative class\n",
    "                if not neg_classes:\n",
    "                    # all categories except the positive ones\n",
    "                    all_cats = coco.loadCats(coco.getCatIds())\n",
    "                    neg_classes = set([cat[\"name\"] for cat in all_cats]) - set(self.classes)                   \n",
    "                imgs_per_class = max(1, np.array(data_lens[split_dir], dtype=np.int64).mean(dtype=np.int64) \\\n",
    "                                / len(neg_classes))\n",
    "                \n",
    "                # The COCO API has an attribute of type dictionary where each category_id key maps to\n",
    "                # all images_ids of that class. Let's use that to make sets.\n",
    "                \n",
    "                # Get the positive image ids, we use sets to avoid duplicates.\n",
    "                pos_classes_img_ids = set()\n",
    "                for cat_id in cat_ids:\n",
    "                    pos_classes_img_ids |= set(coco.catToImgs[cat_id])\n",
    "                \n",
    "                neg_classes_id = coco.getCatIds(neg_classes)\n",
    "                \n",
    "                # Find non negative images that do not contain positives and add `imgs_per_class` from\n",
    "                # each subclass to the negative one.\n",
    "                neg_images_ids = []\n",
    "                for nclass_id in neg_classes_id:\n",
    "                    n_subclass_imgs = set(coco.catToImgs[nclass_id])\n",
    "                    n_subclass_imgs -= pos_classes_img_ids\n",
    "                    imgs_to_sample = imgs_per_class if len(n_subclass_imgs)>imgs_per_class else len(n_subclass_imgs)\n",
    "                    n_subclass_imgs = random.sample(tuple(n_subclass_imgs), int(imgs_to_sample))\n",
    "                    neg_images_ids.extend(n_subclass_imgs)\n",
    "                \n",
    "                neg_imgs_data = coco.loadImgs(neg_images_ids)\n",
    "                N = len(neg_imgs_data)\n",
    "                data_lens[split_dir].append(N)\n",
    "                if only_length:\n",
    "                    continue\n",
    "                \n",
    "                # Download negative images\n",
    "                tic = time.time()\n",
    "                for i, img in enumerate(neg_imgs_data):\n",
    "                    fname = os.path.join(neg_path, img['file_name'])\n",
    "                    if not os.path.exists(fname):\n",
    "                        urllib.request.urlretrieve(img['coco_url'], fname)\n",
    "                    print('Downloaded {}/{} images (t={:.2f}s)'.format(i+1, N, time.time()- tic), end=\"\\r\")\n",
    "                print(\"\\n\")         \n",
    "        return data_lens\n",
    "    \n",
    "    def to_tf_dataset(self):\n",
    "        \"\"\"\n",
    "        Get `tf.dataset` object from all the images downloaded in a convenient format:\n",
    "        \n",
    "        (TODO)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    def to_torch_dataloader(self):\n",
    "        print(\"This function is not yet implemented.\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_c = COCO_MLC(DATA_DIR, coco_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping .\\data\\annotations\\annotations_train2017.zip\n",
      "... done unzipping\n",
      "... done\n",
      "loading annotations into memory...\n",
      "Done (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "Download annotations done\n"
     ]
    }
   ],
   "source": [
    "coco_c.download_annotations(delete_zip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=15.46s)\n",
      "creating index...\n",
      "index created!\n",
      "Downloading train data for person category\n",
      "Downloaded 121/121 images (t=0.02s)\n",
      "\n",
      "Downloading train data for car category\n",
      "Downloaded 92/92 images (t=0.02s)\n",
      "\n",
      "Downloading train data for negative category\n",
      "Downloaded 54/78 images (t=33.49s)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-feb79f2f956d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcoco_c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"person\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"car\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold_area\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-64-52035c101123>\u001b[0m in \u001b[0;36mdownload_images\u001b[1;34m(self, classes, threshold_area, only_length, max_length, add_negative_class, neg_classes)\u001b[0m\n\u001b[0;32m    186\u001b[0m                     \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'file_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                         \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'coco_url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downloaded {}/{} images (t={:.2f}s)'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py37\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coco_c.download_images([\"person\", \"car\"], threshold_area=0.05, max_length=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Coco dataset](https://cocodataset.org/#home)\n",
    "- [CocoAPI for Python3 and Windows](https://github.com/philferriere/cocoapi#egg=pycocotools^&subdirectory=PythonAPI)\n",
    "- [Visual wake word dataset](https://arxiv.org/abs/1906.05721)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
